{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giannicha/aiffel/blob/main/EXP_06_writer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d109c2",
      "metadata": {
        "id": "96d109c2",
        "outputId": "dcd406cf-ae10-4b08-e501-5b09d0b88ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*' #os.getenv(x)함수는 환경 변수x의 값을 포함하는 문자열 변수를 반환 \n",
        "\n",
        "txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n",
        "\n",
        "raw_corpus = [] \n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담기.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
        "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b404ea2",
      "metadata": {
        "id": "3b404ea2"
      },
      "source": [
        "STEP03. 데이터정제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce06436",
      "metadata": {
        "id": "bce06436",
        "outputId": "d6d3a813-4ba1-4ebf-dce0-c8bb4c2b151f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now I've heard there was a secret chord\n",
            "That David played, and it pleased the Lord\n",
            "But you don't really care for music, do you?\n",
            "It goes like this\n",
            "The fourth, the fifth\n",
            "The minor fall, the major lift\n",
            "The baffled king composing Hallelujah Hallelujah\n",
            "Hallelujah\n",
            "Hallelujah\n",
            "Hallelujah Your faith was strong but you needed proof\n"
          ]
        }
      ],
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence)== 0 : continue\n",
        "    if sentence[-1]== ':': continue\n",
        "    \n",
        "    if idx >9 :break\n",
        "    \n",
        "    print(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbd92751",
      "metadata": {
        "id": "fbd92751"
      },
      "outputs": [],
      "source": [
        "import os, re \n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "979f0147",
      "metadata": {
        "id": "979f0147",
        "outputId": "a86613cb-8a85-48da-dbcc-ba17f7ea5585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ],
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip() #일단 모두 소문자로 변환하고 양쪽 공백을 지우기\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 아래 특수문자 기호 양쪽에 공백을 추가\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 공백이 많을 수 있는 부분에는 하나의 공백으로 통일\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # \"\"안에 들어가있는 기호들 외에 공백으로 바꾸기\n",
        "    sentence = sentence.strip() #다시 양쪽 공백을 지우기\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작과 끝에 start와 end 를 추가\n",
        "    return sentence\n",
        "\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e43f253",
      "metadata": {
        "id": "9e43f253",
        "outputId": "f7c2c4e1-8709-4573-9d7d-8fb147f4fef2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<start> now i ve heard there was a secret chord <end>',\n",
              " '<start> that david played , and it pleased the lord <end>',\n",
              " '<start> but you don t really care for music , do you ? <end>',\n",
              " '<start> it goes like this <end>',\n",
              " '<start> the fourth , the fifth <end>',\n",
              " '<start> the minor fall , the major lift <end>',\n",
              " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
              " '<start> hallelujah <end>',\n",
              " '<start> hallelujah <end>',\n",
              " '<start> hallelujah your faith was strong but you needed proof <end>']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \":\": continue\n",
        "    \n",
        "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 저장\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "    corpus.append(preprocessed_sentence)\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[:10]\n",
        "        \n",
        "         "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ae9ef2",
      "metadata": {
        "id": "28ae9ef2"
      },
      "source": [
        "STEP04_평가 데이터셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776be53a",
      "metadata": {
        "id": "776be53a",
        "outputId": "29ac5aab-ac81-46dc-94e7-4613a58c336a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   2   50    5 ...    0    0    0]\n",
            " [   2   17 2639 ...    0    0    0]\n",
            " [   2   36    7 ...    0    0    0]\n",
            " ...\n",
            " [   2  130    5 ...    0    0    0]\n",
            " [ 287   79  162 ...  877  647    3]\n",
            " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f7013050fd0>\n"
          ]
        }
      ],
      "source": [
        "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
        "def tokenize(corpus):\n",
        "    # 12000단어를 기억할 수 있는 tokenizer를 만들기\n",
        "    # 우리는 이미 문장을 정제했으니 filters가 필요없음\n",
        "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꾸기\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
        "    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n",
        "    \n",
        "    tokenizer.fit_on_texts(corpus) #위에서 만든 문장을 토크나이저에 넣어 데이터를 구축 \n",
        "    \n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
        "    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n",
        "    tensor = tokenizer.texts_to_sequences(corpus) \n",
        "    total_data_text = list(tensor)\n",
        "    num_tokens = [len(tokens) for tokens in total_data_text]\n",
        "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
        "    maxlen = int(max_tokens)\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞추기\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=maxlen)  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07106755",
      "metadata": {
        "id": "07106755",
        "outputId": "a70a3f67-7bbf-4780-fb2b-c3d0a8f61e68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : ,\n",
            "5 : i\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ],
      "source": [
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx > 9: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2ab2bd",
      "metadata": {
        "id": "4b2ab2bd",
        "outputId": "a0a49d9a-9a77-4599-fda1-99e7a4038c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0\n",
            "    0    0    0    0    0]\n",
            "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0\n",
            "    0    0    0    0    0]\n"
          ]
        }
      ],
      "source": [
        "#마지막 토큰을 잘르기 위에서 end라고 설정했지만 문장 길이 상 pad인 것이 많다.\n",
        "src_input = tensor[:, :-1]\n",
        "#앞에 start부분을 자르기 \n",
        "tgt_input = tensor[:, 1:]\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da83beb1",
      "metadata": {
        "id": "da83beb1"
      },
      "source": [
        "step04_ 평가 데이터 셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90018dfc",
      "metadata": {
        "id": "90018dfc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                          tgt_input,\n",
        "                                                          test_size=0.2,\n",
        "                                                          shuffle=True, \n",
        "                                                          random_state=34)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9bcebed",
      "metadata": {
        "id": "d9bcebed",
        "outputId": "7bcba31f-a6c4-4a2b-ae0f-db043562d2bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source Train:  (140599, 19)\n",
            "Target Train:  (140599, 19)\n"
          ]
        }
      ],
      "source": [
        "print('Source Train: ', enc_train.shape)\n",
        "print('Target Train: ', dec_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3508a2c2",
      "metadata": {
        "id": "3508a2c2"
      },
      "source": [
        "step05_인공지능 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aa706d",
      "metadata": {
        "id": "a7aa706d"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        \n",
        "        self.embedding = Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "#문장을 토큰으로 했을 때 19이므로 19로 설정\n",
        "embedding_size = 19\n",
        "hidden_size = 2048\n",
        "\n",
        "#여기서 tokenizer.num_words + 1 를 했는데 그 이유는 문장에 없는 pad 가 넣어졌기 때문\n",
        "#문장길이를 모두 통일 하기 위해 가장 긴문장 말고는 모든 토큰이 0으로 들어간 부분 때문\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
        "\n",
        "history = []\n",
        "epochs = 10\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4dd55f",
      "metadata": {
        "id": "bb4dd55f"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8cf8c11",
      "metadata": {
        "id": "c8cf8c11",
        "outputId": "19715778-1ed6-4896-e42d-5d6074ca3965"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 19, 12001), dtype=float32, numpy=\n",
              "array([[[-2.73037731e-05,  1.17473792e-05, -3.45171757e-05, ...,\n",
              "          5.79083535e-05, -4.37425479e-06,  9.86227133e-06],\n",
              "        [ 4.36573373e-06,  1.03899940e-04, -4.96865687e-05, ...,\n",
              "          3.16602454e-05, -5.25521609e-05,  4.63086617e-05],\n",
              "        [ 3.01694654e-05,  1.43008758e-04, -3.44213840e-05, ...,\n",
              "         -2.23296629e-05, -6.49107969e-05,  8.48639247e-05],\n",
              "        ...,\n",
              "        [ 4.63791686e-04,  4.51017055e-04, -4.05240979e-04, ...,\n",
              "          6.10057963e-04, -3.12272401e-04,  3.09568131e-04],\n",
              "        [ 5.04185620e-04,  5.15167776e-04, -4.49760526e-04, ...,\n",
              "          7.19565491e-04, -3.45233944e-04,  3.65017302e-04],\n",
              "        [ 5.38956607e-04,  5.68831398e-04, -4.91960032e-04, ...,\n",
              "          8.15629493e-04, -3.74704745e-04,  4.14770882e-04]],\n",
              "\n",
              "       [[-2.73037731e-05,  1.17473792e-05, -3.45171757e-05, ...,\n",
              "          5.79083535e-05, -4.37425479e-06,  9.86227133e-06],\n",
              "        [-3.09688803e-05,  1.21031502e-04, -9.38183803e-05, ...,\n",
              "          5.36489533e-05, -2.90487824e-05,  6.65043044e-05],\n",
              "        [ 7.17749435e-06,  6.81927413e-05, -9.73369970e-05, ...,\n",
              "          2.92451659e-05,  1.04899937e-05,  9.58401724e-05],\n",
              "        ...,\n",
              "        [-1.83199038e-04, -7.53808345e-05, -2.81259359e-04, ...,\n",
              "         -6.84955239e-06, -1.52376975e-04,  1.81448777e-04],\n",
              "        [-2.77186307e-04, -1.22538826e-04, -2.67618801e-04, ...,\n",
              "         -6.14686724e-05, -1.75733920e-04,  1.46238250e-04],\n",
              "        [-3.03892593e-04, -1.62429162e-04, -2.60120811e-04, ...,\n",
              "         -7.05170023e-05, -1.67712642e-04,  1.35471782e-04]],\n",
              "\n",
              "       [[-2.73037731e-05,  1.17473792e-05, -3.45171757e-05, ...,\n",
              "          5.79083535e-05, -4.37425479e-06,  9.86227133e-06],\n",
              "        [-2.53554117e-05, -9.68436216e-05, -3.30506518e-05, ...,\n",
              "          6.45665350e-05,  2.19894919e-05,  1.18527616e-07],\n",
              "        [-3.60629638e-05, -1.32628964e-04, -1.27454608e-04, ...,\n",
              "          1.78680875e-05, -2.56930816e-05, -6.12172244e-07],\n",
              "        ...,\n",
              "        [-1.21592318e-04,  6.13728247e-04,  1.48717285e-04, ...,\n",
              "         -1.26875675e-04, -2.01955365e-04, -7.87457029e-05],\n",
              "        [-2.05577653e-05,  6.48947025e-04,  8.38743581e-05, ...,\n",
              "          4.81551506e-05, -2.83934874e-04,  1.44654850e-05],\n",
              "        [ 7.91867642e-05,  6.86248182e-04,  9.96429117e-06, ...,\n",
              "          2.18463654e-04, -3.55901924e-04,  1.05755680e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-2.73037731e-05,  1.17473792e-05, -3.45171757e-05, ...,\n",
              "          5.79083535e-05, -4.37425479e-06,  9.86227133e-06],\n",
              "        [-4.50163243e-05,  2.41201560e-05, -1.13659124e-04, ...,\n",
              "         -1.66982263e-05, -2.41986254e-05,  2.97698389e-05],\n",
              "        [-8.71198135e-05, -5.49190872e-06, -1.02812133e-04, ...,\n",
              "          2.10899434e-05,  1.62244796e-05,  1.43256475e-05],\n",
              "        ...,\n",
              "        [ 5.77121624e-04,  5.80748543e-04, -4.79355862e-04, ...,\n",
              "          9.77453077e-04, -3.41732142e-04,  4.23769612e-04],\n",
              "        [ 6.02198939e-04,  6.20424282e-04, -5.26343589e-04, ...,\n",
              "          1.03653362e-03, -3.71868489e-04,  4.61363845e-04],\n",
              "        [ 6.22256659e-04,  6.51183538e-04, -5.67386218e-04, ...,\n",
              "          1.08787254e-03, -3.98478704e-04,  4.96192777e-04]],\n",
              "\n",
              "       [[-2.73037731e-05,  1.17473792e-05, -3.45171757e-05, ...,\n",
              "          5.79083535e-05, -4.37425479e-06,  9.86227133e-06],\n",
              "        [-3.12567608e-05,  1.14136556e-05, -2.13353796e-05, ...,\n",
              "          1.06645639e-04,  3.17363483e-05,  2.67748383e-05],\n",
              "        [-8.43418165e-05,  4.52162240e-05, -1.14468876e-05, ...,\n",
              "          1.23592210e-04,  3.43743050e-05, -2.42909155e-05],\n",
              "        ...,\n",
              "        [ 4.56147129e-04,  6.59112760e-04, -3.11622134e-04, ...,\n",
              "          7.77820766e-04, -3.52270727e-04,  4.08269290e-04],\n",
              "        [ 4.91359853e-04,  6.91652182e-04, -3.77171586e-04, ...,\n",
              "          8.60742875e-04, -3.89841589e-04,  4.50113119e-04],\n",
              "        [ 5.21412003e-04,  7.15287868e-04, -4.36481525e-04, ...,\n",
              "          9.34490701e-04, -4.21080651e-04,  4.88093472e-04]],\n",
              "\n",
              "       [[-2.73037731e-05,  1.17473792e-05, -3.45171757e-05, ...,\n",
              "          5.79083535e-05, -4.37425479e-06,  9.86227133e-06],\n",
              "        [-3.09688803e-05,  1.21031502e-04, -9.38183803e-05, ...,\n",
              "          5.36489533e-05, -2.90487824e-05,  6.65043044e-05],\n",
              "        [-3.03269171e-05,  1.68082959e-04, -1.72533633e-04, ...,\n",
              "          5.63909598e-05, -9.12914402e-05,  9.00811574e-05],\n",
              "        ...,\n",
              "        [ 3.00951360e-04,  5.05819975e-04, -5.12314669e-04, ...,\n",
              "          6.90733199e-04, -3.60759732e-04,  3.26996727e-04],\n",
              "        [ 3.68960551e-04,  5.62265981e-04, -5.45976509e-04, ...,\n",
              "          7.88740173e-04, -3.90118279e-04,  3.74513882e-04],\n",
              "        [ 4.27107123e-04,  6.09508192e-04, -5.78153820e-04, ...,\n",
              "          8.74695485e-04, -4.14417213e-04,  4.18099604e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8d8e13",
      "metadata": {
        "id": "0b8d8e13",
        "outputId": "e3939197-1e1b-45f9-a646-7c9d408f7c74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  228019    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  multiple                  16941056  \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                multiple                  33562624  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  24590049  \n",
            "=================================================================\n",
            "Total params: 75,321,748\n",
            "Trainable params: 75,321,748\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5754c6f2",
      "metadata": {
        "id": "5754c6f2"
      },
      "source": [
        "- 모델적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe871d60",
      "metadata": {
        "id": "fe871d60",
        "outputId": "6ba00b18-99ff-4092-ed3b-ba30fd785e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "550/550 [==============================] - 390s 702ms/step - loss: 3.0848 - val_loss: 2.7216\n",
            "Epoch 2/10\n",
            "550/550 [==============================] - 390s 710ms/step - loss: 2.5785 - val_loss: 2.4813\n",
            "Epoch 3/10\n",
            "550/550 [==============================] - 391s 712ms/step - loss: 2.3752 - val_loss: 2.3450\n",
            "Epoch 4/10\n",
            "550/550 [==============================] - 392s 712ms/step - loss: 2.2226 - val_loss: 2.2498\n",
            "Epoch 5/10\n",
            "550/550 [==============================] - 392s 713ms/step - loss: 2.0758 - val_loss: 2.1750\n",
            "Epoch 6/10\n",
            "550/550 [==============================] - 392s 712ms/step - loss: 1.9367 - val_loss: 2.1174\n",
            "Epoch 7/10\n",
            "550/550 [==============================] - 392s 712ms/step - loss: 1.8031 - val_loss: 2.0678\n",
            "Epoch 8/10\n",
            "550/550 [==============================] - 392s 712ms/step - loss: 1.6726 - val_loss: 2.0270\n",
            "Epoch 9/10\n",
            "550/550 [==============================] - 392s 713ms/step - loss: 1.5483 - val_loss: 1.9973\n",
            "Epoch 10/10\n",
            "550/550 [==============================] - 392s 714ms/step - loss: 1.4308 - val_loss: 1.9784\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(enc_train, \n",
        "          dec_train, \n",
        "          epochs=epochs,\n",
        "          batch_size=256,\n",
        "          validation_data=(enc_val, dec_val),\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94417ac1",
      "metadata": {
        "id": "94417ac1"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성\n",
        "    while True:\n",
        "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 된다. \n",
        "                # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 준다\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "\n",
        "        # 우리 모델이 <END>를 예측하지 않았거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환\n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "172c383f",
      "metadata": {
        "id": "172c383f",
        "outputId": "56edc575-a0dd-4133-fe48-0a72af24c435"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<start> i love you so much , i love you so much , i love you so much <end> '"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91dff514",
      "metadata": {
        "id": "91dff514"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}